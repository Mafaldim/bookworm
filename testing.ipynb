{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def remove_punctuation(input_string):\n",
    "    return input_string.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "\n",
    "def window_over_sentences(sentences, size=2):\n",
    "    return [' '.join(sentences[i : i + size]) \n",
    "            for i in range(len(sentences) - size - 1)]\n",
    "\n",
    "\n",
    "def sequences_to_count_matrix(sequences, count_vectorizer):\n",
    "    '''count instances of each word in the vocabulary in each sentence'''\n",
    "    return count_vectorizer.fit_transform(sequences).todense()\n",
    "\n",
    "\n",
    "def is_plausible_entity(word):\n",
    "    return ((word.pos_ == 'PROPN') & \n",
    "            (word.text.istitle()) & \n",
    "            (len(word) > 2))\n",
    "\n",
    "\n",
    "def get_plausible_entities(count_vectorizer):\n",
    "    vocabulary = remove_punctuation(' '.join(count_vectorizer.vocabulary_.keys()))\n",
    "    plausible_entities = [word.text for word in nlp(vocabulary)\n",
    "                          if is_plausible_entity(word)]\n",
    "    return plausible_entities\n",
    "\n",
    "\n",
    "def get_adjacency_matrix(count_matrix, count_vectorizer, plausible_entities):\n",
    "    '''\n",
    "    count instances of each plausible entity in each sequence. \n",
    "    return character/character counts\n",
    "    '''\n",
    "    relevant_indicies = [count_vectorizer.vocabulary_[e]\n",
    "                         for e in plausible_entities]\n",
    "    interaction_matrix = count_matrix[:, relevant_indicies]\n",
    "    adjacency = interaction_matrix.T.dot(interaction_matrix)\n",
    "    np.fill_diagonal(adjacency, 0)\n",
    "    return pd.DataFrame(data=adjacency, \n",
    "                        columns=plausible_entities, \n",
    "                        index=plausible_entities)\n",
    "\n",
    "\n",
    "def get_edgelist(adjacency, threshold):\n",
    "    rows, columns = np.where(np.triu(adjacency.values, 1) > threshold)\n",
    "    edges = np.column_stack([adjacency.index[rows],\n",
    "                             adjacency.columns[columns],\n",
    "                             adjacency.values[rows, columns]])\n",
    "    return pd.DataFrame(data=edges,\n",
    "                        columns=['source', 'target', 'value'])\n",
    "\n",
    "\n",
    "def bookworm(book, threshold=15):\n",
    "    sentences = nltk.sent_tokenize(book)\n",
    "    sequences = window_over_sentences(sentences)\n",
    "    count_vectorizer = CountVectorizer(lowercase=False)\n",
    "    count_matrix = sequences_to_count_matrix(sequences, count_vectorizer)\n",
    "    plausible_entities = get_plausible_entities(count_vectorizer)\n",
    "\n",
    "    adjacency = get_adjacency_matrix(count_matrix, \n",
    "                                     count_vectorizer, \n",
    "                                     plausible_entities)\n",
    "\n",
    "    edgelist = get_edgelist(adjacency, threshold)\n",
    "    return nx.from_pandas_edgelist(edgelist, \n",
    "                                   source='source', \n",
    "                                   target='target', \n",
    "                                   edge_attr='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import netlsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://www.glozman.com/textpages.html'\n",
    "a = requests.get(base_url).text\n",
    "soup = BeautifulSoup(a, 'html.parser')\n",
    "urls = ['http://www.glozman.com/' + url.get('href') for url in soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_parse = [url for url in urls if 'Harry Potter' in url]\n",
    "graphs = {}\n",
    "\n",
    "for url in tqdm(urls_to_parse):\n",
    "    try:\n",
    "        book = requests.get(url).text\n",
    "        graphs[url] = bookworm(book, threshold=20)\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = {url: netlsd.heat(graph) for url, graph in graphs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "similarity = {\n",
    "    n_1: {n_2: netlsd.compare(sig_1, sig_2)\n",
    "          for n_2, sig_2 in feature_vectors.items()\n",
    "         }\n",
    "    for n_1, sig_1 in feature_vectors.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(data=similarity,\n",
    "                         columns=feature_vectors.keys(),\n",
    "                         index=feature_vectors.keys()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(graphs['http://www.glozman.com/TextPages/Harry Potter 1 - Sorcerer\\'s Stone.txt'],\n",
    "        with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
